{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Epoch 1/3, Loss: 0.3321\n",
       "Epoch 2/3, Loss: 0.1204\n",
       "Epoch 3/3, Loss: 0.0478\n",
       "Saving fine-tuned embeddings...\n",
       "Fine-tuned embeddings saved to 'fine_tuned_vocab_embeddings.npy'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer, TFAutoModel, BertModel, BertTokenizer\n",
    "\n",
    "# Load BERT tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = TFAutoModel.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Example labeled data: (word1, word2, target_word)\n",
    "labeled_data = [\n",
    "    (\"fire\", \"water\", \"steam\"),\n",
    "    (\"fire\", \"earth\", \"lava\"),\n",
    "    (\"fire\", \"wind\", \"smoke\"),\n",
    "    (\"water\", \"earth\", \"plant\"),\n",
    "    (\"water\", \"wind\", \"wave\"),\n",
    "    (\"earth\", \"wind\", \"dust\"),\n",
    "    (\"wind\", \"fire\", \"smoke\"),\n",
    "]\n",
    "\n",
    "# Function to get the embedding of a word\n",
    "def get_embedding(word):\n",
    "    inputs = tokenizer(word, return_tensors=\"tf\")\n",
    "    outputs = model(**inputs)\n",
    "    return tf.reduce_mean(outputs.last_hidden_state, axis=1)  # Mean pooling\n",
    "\n",
    "# Custom cosine similarity loss function\n",
    "def cosine_similarity_loss(combined_embedding, target_embedding):\n",
    "    combined_norm = tf.nn.l2_normalize(combined_embedding, axis=1)\n",
    "    target_norm = tf.nn.l2_normalize(target_embedding, axis=1)\n",
    "    cosine_sim = tf.reduce_sum(combined_norm * target_norm, axis=1)\n",
    "    loss = 1.0 - tf.reduce_mean(cosine_sim)  # Target similarity: 1.0\n",
    "    return loss\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)\n",
    "\n",
    "# Training loop\n",
    "epochs = 3\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "\n",
    "    for word1, word2, target_word in labeled_data:\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Get embeddings for word1, word2, and target_word\n",
    "            embedding1 = get_embedding(word1)\n",
    "            embedding2 = get_embedding(word2)\n",
    "            target_embedding = get_embedding(target_word)\n",
    "\n",
    "            # Combine embeddings (simple addition)\n",
    "            combined_embedding = embedding1 + embedding2\n",
    "\n",
    "            # Compute cosine similarity loss\n",
    "            loss = cosine_similarity_loss(combined_embedding, target_embedding)\n",
    "\n",
    "        # Update model parameters\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "        total_loss += loss.numpy()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss:.4f}\")\n",
    "\n",
    "# Save fine-tuned embeddings\n",
    "vocab = tokenizer.get_vocab()\n",
    "vocab_words = list(vocab.keys())\n",
    "fine_tuned_embeddings = []\n",
    "\n",
    "print(\"Saving fine-tuned embeddings...\")\n",
    "for word in vocab_words:\n",
    "    embedding = get_embedding(word).numpy().squeeze()\n",
    "    fine_tuned_embeddings.append(embedding)\n",
    "\n",
    "# Save to file\n",
    "np.save('fine_tuned_vocab_embeddings.npy', fine_tuned_embeddings)\n",
    "print(\"Fine-tuned embeddings saved to 'fine_tuned_vocab_embeddings.npy'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30522, 768)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tjecking the size of the fine-tuned embeddings\n",
    "fine_tuned_embeddings = np.load('fine_tuned_vocab_embeddings.npy')\n",
    "print(fine_tuned_embeddings.shape)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "The closest word to the combination of 'fire' and 'steam' is: [('smoke', 0.9974934), ('water', 0.9959872), ('wind', 0.99542344), ('jet', 0.9954127), ('sail', 0.99539626)]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def combine_and_find_closest_words1(word1, word2, fine_tuned_embeddings, vocab_words, top_k=5):\n",
    "    embedding1 = get_embedding(word1).numpy().squeeze()\n",
    "    embedding2 = get_embedding(word2).numpy().squeeze()\n",
    "    combined_embed = embedding1 + embedding2\n",
    "\n",
    "    # Compute cosine similarity between the combined embedding and all embeddings\n",
    "    similarities = cosine_similarity([combined_embed], fine_tuned_embeddings)[0]\n",
    "\n",
    "    # Sort indices by similarity in descending order\n",
    "    sorted_indices = similarities.argsort()[::-1]\n",
    "\n",
    "    # Filter out words that match word1 or word2\n",
    "    closest_word = []\n",
    "    for i in sorted_indices:\n",
    "        candidate_word = vocab_words[i]\n",
    "        if candidate_word != word1 and candidate_word != word2:\n",
    "            closest_word.append((candidate_word, similarities[i]))\n",
    "            if len(closest_word) == top_k:  \n",
    "                break\n",
    "\n",
    "    return closest_word\n",
    "\n",
    "    \n",
    "word1 = \"fire\"\n",
    "word2 = \"steam\"\n",
    "closest_word = combine_and_find_closest_words1(word1, word2, fine_tuned_embeddings, vocab_words)\n",
    "print(f\"The closest word to the combination of '{word1}' and '{word2}' is: {closest_word}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "The closest word to the combination of 'tree' and 'candy' is: [('trees', 0.99488986), ('juice', 0.9948405), ('monkey', 0.99459296), ('cake', 0.9945701), ('doll', 0.9944013)]\n",
       "['earth', 'water', 'fire', 'wind', 'trees']\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Initial list of discovered items to combine\n",
    "discovered_items = ['earth', 'water','fire', 'wind']\n",
    "\n",
    "#Choose 2 items to combine\n",
    "item1 = \"tree\"\n",
    "item2 = \"candy\"\n",
    "\n",
    "closest_word = combine_and_find_closest_words1(item1, item2, fine_tuned_embeddings, vocab_words)\n",
    "print(f\"The closest word to the combination of '{item1}' and '{item2}' is: {closest_word}\")\n",
    "# Add the closest word to discovered items\n",
    "new_item = closest_word[0][0]\n",
    "if new_item not in discovered_items:  \n",
    "    discovered_items.append(new_item)\n",
    "\n",
    "print(discovered_items)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
